{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbd699f-b963-4ba4-b9cb-bcdfb1f551cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "cuisines = ['afghan', 'african', 'american','australian', 'asian','austrian',\n",
    "            'azerbaijan', 'balinese', 'belgian','brazilian','cajun-creole',\n",
    "            'caribbean', 'chinese','cuba','danish', 'dinner', 'eastern-european', 'egyptian', 'english',\n",
    "            'finland','french', 'german', 'greek', 'hungarian', 'indian', 'indonesian', 'irish', 'italian', 'jamaican',\n",
    "            'japanese', 'jewish', 'korean', 'latin-american', 'lithuanian', 'mediterranean', 'mexican',\n",
    "            'middle-eastern', 'moroccan', 'nepalese', 'nigerian', 'north-african', 'persian', 'peruvian', 'polish', 'portuguese',\n",
    "            'scandinavian', 'senegalese', 'scottish', 'southern-soul', 'spanish', 'swedish', 'swiss',\n",
    "            'taiwanese', 'thai', 'tunisian', 'turkish', 'ukrainian', 'vietnamese', 'welsh', 'balkan', 'czech', 'czech-cuisine']\n",
    "\n",
    "# dietary= ['vegetarian','vegan', 'gluten-free','nut-free','healthy', 'dairy-free', 'egg-free', 'low-calorie', 'low-sugar',\n",
    "#           'high-protein', 'low-fat', 'high-fibre', 'keto', 'low-carb']\n",
    "\n",
    "base_url= 'https://www.bbcgoodfood.com/search?tab=recipe'\n",
    "\n",
    "\n",
    "def preference_based_search(preference):\n",
    "    '''Return a url for a BBC GoodFood search that\n",
    "    is specific to the input category/preference.'''\n",
    "    if preference in cuisines:\n",
    "        url= f'{base_url}&cuisine={preference}'\n",
    "    # elif preference in dietary:\n",
    "    #     url= f'{base_url}&diet={preference}'\n",
    "    else:\n",
    "        url=base_url\n",
    "    return url\n",
    "\n",
    "def category_specific_links(preference, page_range=45):\n",
    "    '''Given a URL for a category search in BBC GoodFood,\n",
    "    return a list of specific links for each recipe from\n",
    "    the categorical search'''\n",
    "\n",
    "    # if preference in ['vegetarian','healthy', 'gluten-free','british']:\n",
    "    #     page_range=205\n",
    "\n",
    "    recipe_links =[]\n",
    "    for num in range(1, page_range):\n",
    "        try:\n",
    "            search_url = preference_based_search(preference) + f'&page={num}'\n",
    "            response = requests.get(search_url)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            for link in soup.find_all(\"a\", class_=\"link d-block\"):\n",
    "                recipe_links.append(f'https://www.bbcgoodfood.com/recipes{link[\"href\"]}')\n",
    "        except:\n",
    "            recipe_links.append('n')\n",
    "\n",
    "    valid_link =[]\n",
    "    for link in recipe_links:\n",
    "        if 'https' not in link[10:]:\n",
    "            valid_link.append(link)\n",
    "    return list(set(valid_link))\n",
    "\n",
    "\n",
    "\n",
    "def category_bbc_data(preference):\n",
    "    '''Return a dataframe of recipe information for each recipe\n",
    "    given a specific category'''\n",
    "\n",
    "    individual_recipes = category_specific_links(preference)\n",
    "\n",
    "    #list for each item retrieved\n",
    "    recipe_title=[]\n",
    "    prep_times =[]\n",
    "    cooking_times=[]\n",
    "    stars=[]\n",
    "    review_count=[]\n",
    "    difficulty_level=[]\n",
    "    servings=[]\n",
    "    description=[]\n",
    "    recipe_ingredients=[]\n",
    "    only_ingredients=[]\n",
    "\n",
    "    #here the individual recipe links are pulled and iterated\n",
    "    for recipe in individual_recipes:\n",
    "        time.sleep(0.25)\n",
    "        response = requests.get(recipe, allow_redirects=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        #recipe title\n",
    "        try:\n",
    "            title = soup.find(\"div\", class_= \"headline post-header__title post-header__title--masthead-layout\").find(\n",
    "            \"h1\",class_='heading-1').string\n",
    "            recipe_title.append(title)\n",
    "        except:\n",
    "            recipe_title.append('n')\n",
    "\n",
    "        #prep and cooking times\n",
    "        try:\n",
    "            times = soup.find(\"div\", class_= \"icon-with-text__children\").find_all('time')\n",
    "            prep_times.append(times[0].text.strip(' mins'))\n",
    "        except:\n",
    "            prep_times.append('n')\n",
    "\n",
    "        try:\n",
    "            times = soup.find(\"div\", class_= \"icon-with-text__children\").find_all('time')\n",
    "            cooking_times.append(times[1].text.strip(' mins'))\n",
    "        except:\n",
    "            cooking_times.append('n')\n",
    "\n",
    "\n",
    "        try:\n",
    "        #star ratings\n",
    "            star_rating = soup.find(\"div\", class_= \"rating__values\").find(\"span\",class_='sr-only').string\n",
    "            star_rating_float = float(star_rating.strip('A star rating of ').strip(' out of 5.'))\n",
    "            stars.append(star_rating_float)\n",
    "        except:\n",
    "            stars.append('n')\n",
    "\n",
    "        try:\n",
    "        #review count\n",
    "            num_reviews = soup.find(\"div\", class_= \"rating__values\").find(\"span\",class_='rating__count-text body-copy-small').string\n",
    "            num_reviews_float = float(num_reviews.strip(' ratings'))\n",
    "            review_count.append(num_reviews_float)\n",
    "        except:\n",
    "            review_count.append('n')\n",
    "\n",
    "        #servings\n",
    "        try:\n",
    "            serving = soup.find(\"div\", class_= \"icon-with-text post-header__servings body-copy-small body-copy-bold icon-with-text--aligned\").find(\"div\", class_= \"icon-with-text__children\").string\n",
    "        #servings_int = int(serving.strip('Serves '))\n",
    "\n",
    "            servings.append(serving)\n",
    "        except:\n",
    "            servings.append(\"None\")\n",
    "\n",
    "        #description\n",
    "        try:\n",
    "            tagline = soup.find(\"div\", class_= \"editor-content mt-sm pr-xxs hidden-print\").find(\"p\").string\n",
    "            description.append(tagline)\n",
    "        except:\n",
    "            description.append('n')\n",
    "\n",
    "        #difficulty level\n",
    "        try:\n",
    "            difficulty = soup.find(\"div\", class_= \"icon-with-text post-header__skill-level body-copy-small body-copy-bold icon-with-text--aligned\").find(\"div\", class_= \"icon-with-text__children\").string\n",
    "            difficulty_level.append(difficulty)\n",
    "        except:\n",
    "            difficulty_level.append('n')\n",
    "\n",
    "\n",
    "        #ingredients with measurement\n",
    "        try:\n",
    "            for ingredient_group in soup.find_all('section', class_='recipe__ingredients col-12 mt-md col-lg-6'):\n",
    "                ingredients = ingredient_group.find_all('li')\n",
    "                ingredient_text = ''\n",
    "            for ingredient in ingredients:\n",
    "                ingredient_text += ingredient.get_text(',') + ', '\n",
    "            recipe_ingredients.append(ingredient_text)\n",
    "        except:\n",
    "            recipe_ingredients.append('n')\n",
    "\n",
    "        #ingedients alone\n",
    "        try:\n",
    "            for ingred_group in soup.find_all('section', class_='recipe__ingredients col-12 mt-md col-lg-6'):\n",
    "                ingredient = ingred_group.find_all('a')\n",
    "                ingredient_text = ''\n",
    "            for ing in ingredient:\n",
    "                ingredient_text += ing.get_text().strip(',') + ', '\n",
    "            only_ingredients.append(ingredient_text)\n",
    "        except:\n",
    "            only_ingredients.append('n')\n",
    "\n",
    "        #Creating a Dictionary\n",
    "    category_dictionary= {\n",
    "        'recipe_title':recipe_title,\n",
    "        'stars':stars,\n",
    "        'prep_times':prep_times,\n",
    "        'cooking_times': cooking_times,\n",
    "        'review_count':review_count,\n",
    "        'difficulty_level':difficulty_level,\n",
    "        'servings':servings,\n",
    "        'description':description,\n",
    "        'specific_ingredients':recipe_ingredients,\n",
    "        'ingredients':only_ingredients\n",
    "    }\n",
    "\n",
    "    #Dictionary to dataframe\n",
    "    df = pd.DataFrame.from_dict(category_dictionary)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_image(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    image_url = soup.find('img')\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    '''merging the dataframes together for the entire dataframe used in\n",
    "    the replenish modelling and project'''\n",
    "\n",
    "    print(\"Scraping cuisines from BBC Good Foods\")\n",
    "    all_df_lst = []\n",
    "\n",
    "    for cuisine in cuisines:\n",
    "        df = category_bbc_data(cuisine)\n",
    "        df['preference']=cuisine\n",
    "        all_df_lst.append(df)\n",
    "\n",
    "    # final_df = pd.concat(all_df_lst)\n",
    "    print('Scraping done')\n",
    "    # print(\"Scraping dietary recipes from BBC Good Foods\")\n",
    "\n",
    "    # for diet in dietary:\n",
    "    #     df = category_bbc_data(diet)\n",
    "    #     df['preference']=diet\n",
    "    #     all_df_lst.append(df)\n",
    "\n",
    "    final_df = pd.concat(all_df_lst)\n",
    "\n",
    "    final_df.reset_index(inplace=True)\n",
    "    print('Finished scraping all')\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f66cdb9-b17c-4f52-bd11-619e26115c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping cuisines from BBC Good Foods\n",
      "Scraping done\n",
      "Finished scraping all\n"
     ]
    }
   ],
   "source": [
    "df_non_brit = load_data()\n",
    "df_non_brit.to_csv('non_brit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7665f739-de7b-46e7-89fc-7246e7a67751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/camillemolen/code/mfaruki/replenish'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6d1d6-1461-439f-83f3-8ffc222b7584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
